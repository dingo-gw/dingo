data:
  waveform_dataset_path: /home/jwildberger/Desktop/dingo-devel/tutorials/02_gwpe/datasets/waveforms/waveform_dataset.hdf5  # Contains intrinsic waveforms
  train_fraction: 0.95
  window:  # Needed to calculate window factor for simulated data
    type: tukey
    f_s: 4096
    T: 8.0
    roll_off: 0.4
  domain_update:
    f_min: 20.0
    f_max: 1024.0
  svd_size_update: 200  # Optionally, reduce the SVD size when decompressing (for performance)
  detectors:
    - H1
    - L1
  extrinsic_prior:  # Sampled at train time
    dec: default
    ra: default
    geocent_time: bilby.core.prior.Uniform(minimum=-0.10, maximum=0.10)
    psi: default
    luminosity_distance: bilby.core.prior.Uniform(minimum=100.0, maximum=1000.0)
  ref_time: 1126259462.391
  gnpe_time_shifts:
    kernel: bilby.core.prior.Uniform(minimum=-0.001, maximum=0.001)
    exact_equiv: True
  gnpe_chirp:
    order: 2
    kernel:
      chirp_mass: bilby.core.prior.Uniform(minimum=0.98, maximum=1.02)
      mass_ratio: bilby.core.prior.Uniform(minimum=0.95, maximum=1.05)
  gnpe_phase:
    kernel: bilby.core.prior.Uniform(minimum=-0.1, maximum=0.1)
    random_pi_jump: True
  inference_parameters: default # [chirp_mass, mass_ratio,  luminosity_distance, dec]

# Model architecture
model:
  type: nsf+embedding
  # kwargs for neural spline flow
  nsf_kwargs:
    num_flow_steps: 3 # 30
    base_transform_kwargs:
      hidden_dim: 64 # 512
      num_transform_blocks: 5
      activation: elu
      dropout_probability: 0.0
      batch_norm: True
      num_bins: 8
      base_transform_type: rq-coupling
  # kwargs for embedding net
  embedding_net_kwargs:
    output_dim: 128
#      hidden_dims: [1024, 1024, 1024, 1024, 1024, 1024,
#                    512, 512, 512, 512, 512, 512,
#                    256, 256, 256, 256, 256, 256,
#                    128, 128, 128, 128, 128, 128]
    hidden_dims: [1024, 512, 256, 128]
    activation: elu
    dropout: 0.0
    batch_norm: True
    svd:
      num_training_samples: 20000
      num_validation_samples: 5000
      size: 200

# Training is divided in stages. They each require all settings as indicated below.
training:
  stage_0:
    epochs: 300
    asd_dataset_path: /home/jwildberger/Desktop/dingo-devel/tutorials/02_gwpe/datasets/ASDs_new/asds_O3_fiducial.hdf5
    freeze_rb_layer: True
    optimizer:
      type: adam
      lr: 0.0001
    scheduler:
      type: cosine
      T_max: 300
    batch_size: 64

  stage_1:
    epochs: 150
    asd_dataset_path: ./asds_O3.hdf5
    freeze_rb_layer: False
    optimizer:
      type: adam
      lr: 0.00001
    scheduler:
      type: cosine
      T_max: 150
    batch_size: 64

# Local settings for training that have no impact on the final trained network.
local:
  device: cpu
  num_workers: 6  # num_workers >0 does not work on Mac, see https://stackoverflow.com/questions/64772335/pytorch-w-parallelnative-cpp206
  use_wandb: True
  wandb_api_key: # insert_here. Can be obtained from https://wandb.ai/settings
  runtime_limits:
    max_time_per_run: 36000
    max_epochs_per_run: 500
  checkpoint_epochs: 10
  condor:
    bid: 100
    num_cpus: 16
    memory_cpus: 128000
    num_gpus: 1
    memory_gpus: 8000
