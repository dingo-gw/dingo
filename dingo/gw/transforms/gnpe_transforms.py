import numpy as np
import torch
import lal
from bilby.core.prior import PriorDict
from abc import ABC, abstractmethod

from dingo.gw.domains import FrequencyDomain, Domain


class GNPEBase(ABC):
    """
    A base class for Group Equivariant Neural Posterior Estimation [1].

    This implements GNPE for *approximate* equivariances. For exact equivariances,
    additional processing should be implemented within a subclass.

    [1]: https://arxiv.org/abs/2111.13139
    """

    def __init__(self, kernel_dict, operators):
        self.kernel = PriorDict(kernel_dict)
        self.operators = operators
        self.proxy_list = [k + "_proxy" for k in kernel_dict.keys()]
        self.input_parameter_names = list(self.kernel.keys())

    @abstractmethod
    def __call__(self, input_sample):
        pass

    def sample_proxies(self, input_parameters):
        """
        Given input parameters, perturbs based on the
        kernel to produce "proxy" ("hatted") parameters, i.e., samples

            \hat g ~ p(\hat g | g).

        Typically the GNPE NDE will be conditioned on \hat g. Furthermore, these proxy
        parameters will be used to transform the data to simplify it.

        Parameters:
        -----------
        input_parameters : dict
            Initial parameter values to be perturbed. dict values can be either floats
            (for training) or torch Tensors (for inference).

        Returns
        -------
        A dict of proxy parameters.
        """
        proxies = {}
        for k in self.kernel:
            if k not in input_parameters:
                raise KeyError(
                    f"Input parameters are missing key {k} required for GNPE."
                )
            g = input_parameters[k]
            g_hat = self.perturb(g, k)
            proxies[k + "_proxy"] = g_hat
        return proxies

    def perturb(self, g, k):
        """
        Generate proxy variables based on initial parameter values.

        Parameters
        ----------
        g : Union[np.float64, float, torch.Tensor]
            Initial parameter values
        k : str
            Parameter name. This is used to identify the group binary operator.

        Returns
        -------
        Proxy variables in the same format as g.
        """
        # First we sample from the kernel, ensuring the correct data type,
        # and accounting for possible batching.
        #
        # Batching is implemented only for torch Tensors (expected at inference time),
        # whereas un-batched data in float form is expected during training.
        if type(g) == torch.Tensor:
            epsilon = self.kernel[k].sample(len(g))
            epsilon = torch.tensor(epsilon, dtype=g.dtype, device=g.device)
        elif type(g) == np.float64 or type(g) == float:
            epsilon = self.kernel[k].sample()
        else:
            raise NotImplementedError(f"Unsupported data type {type(g)}.")

        return self.multiply(g, epsilon, k)

    def multiply(self, a, b, k):
        op = self.operators[k]
        if op == "+":
            return a + b
        elif op == "x":
            return a * b
        else:
            raise NotImplementedError(
                f"Unsupported group multiplication operator: {op}"
            )

    def inverse(self, a, k):
        op = self.operators[k]
        if op == "+":
            return -a
        elif op == "x":
            return 1 / a
        else:
            raise NotImplementedError(
                f"Unsupported group multiplication operator: {op}"
            )


class GNPECoalescenceTimes(GNPEBase):
    """
    GNPE [1] Transformation for detector coalescence times.

    For each of the detector coalescence times, a proxy is generated by adding a
    perturbation epsilon from the GNPE kernel to the true detector time. This proxy is
    subtracted from the detector time, such that the overall time shift only amounts to
    -epsilon in training. This standardizes the input data to the inference network,
    since the applied time shifts are always restricted to the range of the kernel.

    To preserve information at inference time, conditioning of the inference network on
    the proxies is required. To that end, the proxies are stored in sample[
    'gnpe_proxies'].

    We can enforce an exact equivariance under global time translations, by subtracting
    one proxy (by convention: the first one, usually for H1 ifo) from all other
    proxies, and from the geocent time, see [1]. This is enabled with the flag
    exact_global_equivariance.

    Note that this transform does not modify the data itself. It only determines the
    amount by which to time-shift the data.

    [1]: arxiv.org/abs/2111.13139
    """

    def __init__(
        self, ifo_list, kernel, exact_global_equivariance=True, inference=False
    ):
        """
        Parameters
        ----------
        ifo_list : bilby.gw.detector.InterferometerList
            List of interferometers.
        kernel : str
            Defines a Bilby prior, to be used for all interferometers.
        exact_global_equivariance : bool = True
            Whether to impose the exact global time translation symmetry.
        inference : bool = False
            Whether to use inference or training mode.
        """
        self.ifo_time_labels = [ifo.name + "_time" for ifo in ifo_list]
        kernel_dict = {k: kernel for k in self.ifo_time_labels}
        operators = {k: "+" for k in self.ifo_time_labels}
        super().__init__(kernel_dict, operators)

        self.inference = inference
        self.exact_global_equivariance = exact_global_equivariance
        if self.exact_global_equivariance:
            del self.proxy_list[0]

    def __call__(self, input_sample):
        sample = input_sample.copy()
        extrinsic_parameters = sample["extrinsic_parameters"].copy()
        new_parameters = self.sample_proxies(extrinsic_parameters)

        # If we are in training mode, we assume that the time shifting due to different
        # arrival times of the signal in individual detectors has not yet been applied
        # to the data; instead the arrival times are stored in extrinsic_parameters.
        # Hence we subtract off the proxy times from these arrival times, so that time
        # shifting of the data only has to be done once.
        if not self.inference:
            for k in self.ifo_time_labels:
                new_parameters[k] = (
                    -new_parameters[k + "_proxy"] + extrinsic_parameters[k]
                )
        # In inference mode, the data are only time shifted by minus the proxy.
        else:
            for k in self.ifo_time_labels:
                new_parameters[k] = -new_parameters[k + "_proxy"]

        # If we are imposing the global time shift symmetry, then we treat the first
        # proxy as "preferred", in the sense that it defines the global time shift.
        # This symmetry is enforced as follows:
        #
        #    1) Do not explicitly condition the model on the preferred proxy
        #    2) Subtract the preferred proxy from geocent_time (assumed to be a regression
        #    parameter). Note that this must be undone at inference time.
        #    3) Subtract the preferred proxy from the remaining proxies. These remaining
        #    proxies then define time shifts relative to the global time shift.
        #
        # Imposing the global time shift does not impact the transformation of the
        # data: we do not change the values of the true detector coalescence times
        # stored in extrinsic_parameters, only the proxies.
        if self.exact_global_equivariance:
            dt = new_parameters.pop(self.ifo_time_labels[0] + "_proxy")
            if not self.inference:
                if "geocent_time" not in extrinsic_parameters:
                    raise KeyError(
                        "geocent_time should be in extrinsic_parameters at "
                        "this point during training."
                    )
                new_parameters["geocent_time"] = (
                    extrinsic_parameters["geocent_time"] - dt
                )
            else:
                new_parameters["geocent_time"] = -dt
            for k in self.ifo_time_labels[1:]:
                new_parameters[k + "_proxy"] -= dt

        extrinsic_parameters.update(new_parameters)
        sample["extrinsic_parameters"] = extrinsic_parameters
        return sample


class GNPEChirp(GNPEBase):
    """
    Relative binning / heterodyning GNPE transform, which factors out the overall chirp
    from the waveform.

    At leading order, given a chirp mass estimate, this applies a multiplicative blur
    to obtain a GNPE proxy variable. The data are then transformed by dividing by a
    fiducial waveform of the form

    exp( - 1j * (3/128) * (pi G chirp_mass_proxy f / c**3)**(-5/3) ) ;

    see 2001.11412, eq. (7.2). This is the leading order chirp due to the emission of
    quadrupole radiation.

    This transform also optionally implements 1PN corrections involving the mass ratio.
    We do not include any amplitude in the fiducial waveform, since at inference time
    this transform will be applied to noisy data. Multiplying the frequency-domain
    noise by a complex number of unit norm is allowed because it only changes the
    phase, not the overall amplitude, which would change the noise PSD.
    """

    def __init__(self, kernel, domain: Domain, order: int = 0):
        """
        Parameters
        ----------
        kernel : dict or str
            Defines a Bilby prior. If a dict, keys should include chirp_mass,
            and (possibly) mass_ratio.
        domain : Domain
            Only works for a FrequencyDomain at present.
        order : int
            Twice the post-Newtonian order for the expansion. Valid orders are 0 and 2.
        """
        if type(kernel) == str:
            # Assume this is the kernel for the chirp mass (backward compatibility)
            kernel = {"chirp_mass": kernel}
        else:
            # We copy the kernel because the PriorDict constructor modifies the argument.
            kernel = kernel.copy()

        if order == 0:
            if "chirp_mass" not in kernel:
                raise KeyError(f"Kernel must include chirp_mass key.")
            if "mass_ratio" in kernel:
                print(
                    "Warning: mass_ratio kernel provided, but will be ignored for "
                    "order 0 GNPE."
                )
                kernel.pop("mass_ratio")
        elif order == 2:
            if "chirp_mass" not in kernel or "mass_ratio" not in kernel:
                raise KeyError(f"Kernel must include chirp_mass and mass_ratio keys.")
        else:
            raise ValueError(f"Order {order} invalid. Acceptable values are 0 and 2.")
        self.order = order

        operators = {"chirp_mass": "x", "mass_ratio": "x"}
        super().__init__(kernel, operators)
        self.domain = domain

    def __call__(self, input_sample):
        sample = input_sample.copy()
        extrinsic_parameters = sample["extrinsic_parameters"].copy()

        # The relevant parameters could be in either the intrinsic or extrinsic
        # parameters list. At inference time, we put all GNPE parameters into the
        # extrinsic parameters list.
        proxies = self.sample_proxies(
            {**sample["parameters"], **sample["extrinsic_parameters"]}
        )
        extrinsic_parameters.update(proxies)
        sample["extrinsic_parameters"] = extrinsic_parameters

        # The only situation where we would expect to not have a waveform to transform
        # would be when calculating parameter standardizations, since we just want to
        # draw samples of the parameters at that point, and not prepare any data.
        if "waveform" in sample:
            sample["waveform"] = self.factor_fiducial_waveform(
                sample["waveform"],
                proxies["chirp_mass_proxy"],
                proxies.get("mass_ratio_proxy"),
            )

        return sample

    def factor_fiducial_waveform(self, data, chirp_mass, mass_ratio):
        """
        Divides the data by the fiducial waveform defined by the chirp mass and (
        optionally) mass ratio. Allows for batching.

        Parameters
        ----------
        data : Union[dict, torch.Tensor]
            If a dict, the keys would correspond to different detectors or
            polarizations. For a Tensor, these would be within different components.
            This method uses the same fiducial waveform for each detector.
        chirp_mass : Union[np.array, torch.Tensor]
        mass_ratio : Union[np.array, torch.Tensor]

        Returns
        -------
        dict or torch.Tensor of the same form as data.
        """
        if isinstance(self.domain, FrequencyDomain):
            if type(data) == dict:
                f = self.domain.get_sample_frequencies_astype(list(data.values())[0])
            else:
                f = self.domain.get_sample_frequencies_astype(data)

            # Expand across possible batch dimension.
            if type(chirp_mass) == np.float64 or type(chirp_mass) == float:
                mc_f = chirp_mass * f
                # Avoid taking a negative power of 0 in the first index. This will get
                # chopped off or multiplied by 0 later anyway.
                if f[0] == 0.0:
                    mc_f[0] = 1.0
            elif type(chirp_mass) == torch.Tensor:
                mc_f = torch.outer(chirp_mass, f)
                if f[0] == 0.0:
                    mc_f[0] = 1.0
                if mass_ratio is not None:
                    mass_ratio = mass_ratio[:, None]
            else:
                raise TypeError(
                    f"Invalid type {type(chirp_mass)}. "
                    f"Only implemented for floats and tensors"
                )

            # Leading (0PN) phase
            pi_mc_f_SI = np.pi * (lal.GMSUN_SI / lal.C_SI ** 3) * mc_f
            fiducial_phase = (3 / 128) * (pi_mc_f_SI) ** (-5 / 3)

            # 1PN correction
            if self.order >= 2:
                assert mass_ratio is not None
                symmetric_mass_ratio = mass_ratio / (1 + mass_ratio) ** 2
                pi_m_f_SI = pi_mc_f_SI / symmetric_mass_ratio ** (3 / 5)
                correction = 1 + (55 * symmetric_mass_ratio / 9 + 3715 / 756) * (
                    pi_m_f_SI
                ) ** (2 / 3)

                fiducial_phase *= correction

            if type(data) == dict:
                result = {}
                for k, v in data.items():
                    result[k] = self.domain.add_phase(v, -fiducial_phase)
            else:
                result = self.domain.add_phase(data, -fiducial_phase)

            return result

        else:
            raise NotImplementedError("Can only use GNPEChirp in frequency domain.")


class GNPEPhase(GNPEBase):
    """GNPE transform for phase of coalescence.

    This transform simply adds a phase proxy parameter to the extrinsic parameters. It
    does not bother to transform the data itself, since this would not provide a
    simpler representation. The point is to guide the networks to identify the phase
    during training by conditioning on the proxy.

    At inference time, the joint posterior over phase and phase_proxy is obtained
    using Gibbs sampling.
    """

    def __init__(self, kernel, random_pi_jump):
        """
        Parameters
        ----------
        kernel : str
            Bilby prior to be used as the GNPE kernel (additive).
        random_pi_jump : bool
            Whether to also include a random jump by pi when sampling the proxy. This
            is useful because the posterior over phase is often multimodal with period
            pi. With this set to True, Gibbs sampling may better explore the space.
        """
        kernel_dict = {"phase": kernel}
        operators = {"phase": "+"}
        super().__init__(kernel_dict, operators)
        self.random_pi_jump = random_pi_jump

    def __call__(self, input_sample):
        sample = input_sample.copy()
        extrinsic_parameters = sample["extrinsic_parameters"].copy()
        proxies = self.sample_proxies(
            {**sample["parameters"], **sample["extrinsic_parameters"]}
        )
        if self.random_pi_jump:
            if isinstance(proxies["phase_proxy"], torch.Tensor):
                proxies["phase_proxy"] += (
                    torch.randint_like(proxies["phase_proxy"], 2) * np.pi
                )
            elif isinstance(proxies["phase_proxy"], (float, np.float64)):
                proxies["phase_proxy"] += np.random.choice([0.0, np.pi])
            else:
                raise NotImplementedError(
                    f"Unsupported data type {type(proxies['phase_proxy'])}."
                )
        proxies["phase_proxy"] = proxies["phase_proxy"] % (2 * np.pi)
        extrinsic_parameters.update(proxies)
        sample["extrinsic_parameters"] = extrinsic_parameters
        return sample
